{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3bbb139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bernardscott/.local/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "import pandas as pd\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load as load_metric\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class PrintStepCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 10 == 0:\n",
    "            loss = state.log_history[-1].get('loss', 'N/A') if state.log_history else 'N/A'\n",
    "            print(f\"Step {state.global_step}, Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13d705db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_row_text(s):\n",
    "    lst = ast.literal_eval(s)\n",
    "    json_str = json.dumps(lst)\n",
    "    obj = json.loads(json_str)\n",
    "    return obj[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fde2ddcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================\n",
    "# 0. Prepare Data\n",
    "# ================================================================\n",
    "FILE_NAME = 'us_smallest_claims_1985_1990_top500'\n",
    "CSV_PATH = './' + FILE_NAME + '.csv'\n",
    "JSON_PATH = './' + FILE_NAME + '.json'\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.drop(columns=['n_claims', 'publication_number', 'publication_date', 'claims_localized_html'], inplace=True)\n",
    "df = df.map(clean_row_text)\n",
    "df.to_json(JSON_PATH, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e93175cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TITLE = 'title'\n",
    "DESCRIPTION = 'description'\n",
    "SUMMARY = 'summary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2eeafab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 patents with real summaries\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 1. Load Data\n",
    "# ================================================================\n",
    "# DATA_PATH = \"./us_smallest_claims_1985_1990_top500.json\"\n",
    "DATA_PATH = \"./batch_summarized.json\"\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data = data[:10]\n",
    "\n",
    "print(f\"Loaded {len(data)} patents with real summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a845336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1: Vehicular turn signal apparatus\n",
      "  Summary: Vehicular turn signal apparatus is provided for securement to a rear shelf surface proximate a rear window of a vehicular interior. The apparatus incl...\n",
      "\n",
      "Example 2: Apparatus for facilitating the machining of workpieces\n",
      "  Summary: Apparatus for facilitating the machining of workpieces. Means to gain access to the cutting edge of a saw tooth in the direction of both the back and ...\n",
      "\n",
      "Example 3: Control valve\n",
      "  Summary: A 3/2 proportional control valve is provided with an actuating piston which is subjected to the control pressure set in a pilot valve. The area relati...\n",
      "\n",
      "Total records: 10\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "for i, item in enumerate(data):\n",
    "    if SUMMARY in item and DESCRIPTION in item:\n",
    "        records.append({\n",
    "            DESCRIPTION: item[DESCRIPTION],\n",
    "            TITLE: item[TITLE],\n",
    "            SUMMARY: item[SUMMARY]\n",
    "        })\n",
    "        if i < 3:\n",
    "            print(f\"\\nExample {i+1}: {item[TITLE]}\")\n",
    "            print(f\"  Summary: {item[SUMMARY][:150]}...\")\n",
    "\n",
    "print(f\"\\nTotal records: {len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a5ac2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DEVICE: mps\n"
     ]
    }
   ],
   "source": [
    "target_device = ''\n",
    "if torch.backends.mps.is_available():\n",
    "    target_device = 'mps'\n",
    "elif torch .cuda.is_available():\n",
    "    target_device = 'cuda'\n",
    "else:\n",
    "    target_device = 'cpu'\n",
    "\n",
    "DEVICE = torch.device(target_device)\n",
    "print(\"Using DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e23c1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 2. Load Model/Tokenizer\n",
    "# ================================================================\n",
    "model_name = \"Qwen/Qwen3-0.6B-Base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    # use bfloat16 only if CUDA is available; otherwise use float32\n",
    "    torch_dtype=(torch.bfloat16 if torch.cuda.is_available() else torch.float32),\n",
    ")\n",
    "model.to(DEVICE)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a74e6883",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "MAX_TEXT_TOKENS = 175"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3878e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train: 8, Test: 2\n",
      "{'description': 'A reliable, pulse-flow supplemental oxygen apparatus for alleviating respiratory ailments is provided which yields substantial savings in oxygen while giving the patient the physiological equivalent of a prescribed continuous stream of oxygen. The apparatus preferably includes a demand oxygen valve operated in a pulse mode by means of electronic control circuitry which, through an appropriate sensor, monitors the patient&#39;s breathing efforts and gives a variable &#34;custom tailored&#34; pulse volume of oxygen to the patient during the very initial stages of each inspiration.', 'title': 'Inspiration oxygen saver', 'summary': 'A reliable, pulse-flow supplemental oxygen apparatus for alleviating respiratory ailments is provided. It yields substantial savings in oxygen while giving the patient the physiological equivalent of a prescribed continuous stream of oxygen. It preferably includes a demand oxygen valve operated in a pulse mode.'}\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 3. Create Dataset\n",
    "# ================================================================\n",
    "\n",
    "def truncate_text(text, max_tokens):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return text\n",
    "\n",
    "dataset = Dataset.from_list(records)\n",
    "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "print(f\"\\nTrain: {len(dataset['train'])}, Test: {len(dataset['test'])}\")\n",
    "print(dataset['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "361d4efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8/8 [00:00<00:00, 429.28 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid label tokens: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 4. Preprocessing\n",
    "# ================================================================\n",
    "def preprocess(batch):\n",
    "    input_ids_list = []\n",
    "    attention_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for summary, description in zip(batch[SUMMARY], batch[DESCRIPTION]):\n",
    "        summary = truncate_text(summary, MAX_TEXT_TOKENS)\n",
    "        \n",
    "        # prompt = f\"Summarize this patent:\\n\\n{text}\\n\\nSummary: \"\n",
    "        prompt = f\"Generate a full detailed patent document based on this summary: \\n\\n{summary}\\n\\n Patent Document:\"\n",
    "        target = description + tokenizer.eos_token\n",
    "        full_text = prompt + target\n",
    "\n",
    "        target_ids = tokenizer.encode(target, add_special_tokens=False)\n",
    "        target_len = len(target_ids)\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        seq_len = sum(attention_mask)\n",
    "\n",
    "        labels = [-100] * MAX_LENGTH\n",
    "        target_start = seq_len - target_len\n",
    "        \n",
    "        for i in range(target_len):\n",
    "            pos = target_start + i\n",
    "            if 0 <= pos < MAX_LENGTH:\n",
    "                labels[pos] = input_ids[pos]\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_list,\n",
    "        \"labels\": labels_list\n",
    "    }\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(\n",
    "    preprocess, \n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "example = tokenized_train[0]\n",
    "valid_count = sum(1 for l in example[\"labels\"] if l != -100)\n",
    "print(f\"Valid label tokens: {valid_count}\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch], dtype=torch.long),\n",
    "        \"labels\": torch.tensor([x[\"labels\"] for x in batch], dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7dbeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NEW_TOKENS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2856dbe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['length_penalty']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline evaluation...\n",
      "Baseline ROUGE-L: 0.0362\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 5. Baseline Evaluation\n",
    "# ================================================================\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "def generate_summary(mdl, text):\n",
    "    text = truncate_text(text, MAX_TEXT_TOKENS)\n",
    "    # prompt = f\"Summarize this patent:\\n\\n{text}\\n\\nSummary:\"\n",
    "    prompt = f\"Generate a full patent document based on this summary: \\n\\n{text}\\n\\n Patent Document:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}  # move to DEVICE (CPU or GPU)\n",
    "\n",
    "    mdl.eval()\n",
    "    with torch.no_grad():\n",
    "        output = mdl.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    full_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    if \"Patent Document:\" in full_output:\n",
    "        return full_output.split(\"Patent Document:\")[-1].strip()\n",
    "    return full_output\n",
    "\n",
    "# test_refs = [item[SUMMARY] for item in dataset[\"test\"]]\n",
    "test_refs = [item[DESCRIPTION] for item in dataset[\"test\"]]\n",
    "\n",
    "print(\"\\nBaseline evaluation...\")\n",
    "baseline_preds = []\n",
    "for item in dataset[\"test\"]:\n",
    "    pred = generate_summary(model, item[DESCRIPTION])\n",
    "    baseline_preds.append(pred)\n",
    "\n",
    "baseline_rouge = rouge.compute(predictions=baseline_preds, references=test_refs)\n",
    "print(f\"Baseline ROUGE-L: {baseline_rouge['rougeL']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e330dbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/CSC9010/lib/python3.13/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "trainable params: 40,370,176 || all params: 636,420,096 || trainable%: 6.3433\n",
      "\n",
      "=== Training ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/50 06:58 < 20:54, 0.03 it/s, Epoch 13/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.386600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, Loss: N/A\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     32\u001b[39m trainer = Trainer(\n\u001b[32m     33\u001b[39m     model=model,\n\u001b[32m     34\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     callbacks=[PrintStepCallback()]\n\u001b[32m     38\u001b[39m )\n\u001b[32m     40\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== Training ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CSC9010/lib/python3.13/site-packages/transformers/trainer.py:2328\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2327\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2333\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CSC9010/lib/python3.13/site-packages/transformers/trainer.py:2672\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2665\u001b[39m context = (\n\u001b[32m   2666\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2667\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2668\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2670\u001b[39m )\n\u001b[32m   2671\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2674\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2675\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2676\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2677\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2678\u001b[39m ):\n\u001b[32m   2679\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2680\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CSC9010/lib/python3.13/site-packages/transformers/trainer.py:4060\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4057\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4058\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4060\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4062\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CSC9010/lib/python3.13/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CSC9010/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CSC9010/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/CSC9010/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 6. LoRA Fine-tuning\n",
    "# ================================================================\n",
    "config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen_lora_patent_real\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.1,\n",
    "    weight_decay=0.01,\n",
    "    bf16=torch.cuda.is_available(),               # enable bf16 only on CUDA\n",
    "    dataloader_pin_memory=torch.cuda.is_available(),  # only pin when CUDA available\n",
    "    logging_steps=10,\n",
    "    save_steps=999999,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=collate_fn,\n",
    "    callbacks=[PrintStepCallback()]\n",
    ")\n",
    "\n",
    "print(\"\\n=== Training ===\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508fbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final evaluation...\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 7. Final Evaluation\n",
    "# ================================================================\n",
    "print(\"\\nFinal evaluation...\")\n",
    "finetuned_preds = []\n",
    "\n",
    "model.eval()\n",
    "for item in dataset[\"test\"]:\n",
    "    pred = generate_summary(model, item[DESCRIPTION])\n",
    "    finetuned_preds.append(pred)\n",
    "\n",
    "finetuned_rouge = rouge.compute(predictions=finetuned_preds, references=test_refs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64758677-d4ae-48b3-8f6c-28290e11977d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SAVING MODEL\n",
      "======================================================================\n",
      "✓ LoRA adapters saved\n",
      "✓ Tokenizer saved\n",
      "✓ Metadata saved\n",
      "✓ Sample predictions saved\n",
      "\n",
      "✓ Complete model package saved to: ./qwen_lora_patent_real/\n",
      "\n",
      "Saved files:\n",
      "  - adapter_model.bin (~13MB)\n",
      "  - adapter_config.json\n",
      "  - tokenizer files\n",
      "  - metadata.json\n",
      "  - sample_predictions.json\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 8. SAVE MODEL PROPERLY\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SAVING MODEL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(\"./qwen_lora_patent_real\")\n",
    "print(\"✓ LoRA adapters saved\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"./qwen_lora_patent_real\")\n",
    "print(\"✓ Tokenizer saved\")\n",
    "\n",
    "# Save comprehensive metadata\n",
    "metadata = {\n",
    "    \"model_info\": {\n",
    "        \"base_model\": model_name,\n",
    "        \"model_type\": \"LoRA_fine-tuned\",\n",
    "        \"task\": \"patent_summarization\"\n",
    "    },\n",
    "    \"training_info\": {\n",
    "        \"training_date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"dataset\": DATA_PATH,\n",
    "        \"num_train_examples\": len(dataset[\"train\"]),\n",
    "        \"num_test_examples\": len(dataset[\"test\"]),\n",
    "        \"num_epochs\": 10,\n",
    "        \"batch_size\": 2,\n",
    "        \"gradient_accumulation_steps\": 4,\n",
    "        \"effective_batch_size\": 8,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"max_length\": MAX_LENGTH,\n",
    "        \"max_text_tokens\": MAX_TEXT_TOKENS\n",
    "    },\n",
    "    \"lora_config\": {\n",
    "        \"r\": 32,\n",
    "        \"lora_alpha\": 64,\n",
    "        \"lora_dropout\": 0.05,\n",
    "        \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    },\n",
    "    \"results\": {\n",
    "        \"baseline\": {\n",
    "            \"rouge1\": float(baseline_rouge['rouge1']),\n",
    "            \"rouge2\": float(baseline_rouge['rouge2']),\n",
    "            \"rougeL\": float(baseline_rouge['rougeL'])\n",
    "        },\n",
    "        \"finetuned\": {\n",
    "            \"rouge1\": float(finetuned_rouge['rouge1']),\n",
    "            \"rouge2\": float(finetuned_rouge['rouge2']),\n",
    "            \"rougeL\": float(finetuned_rouge['rougeL'])\n",
    "        },\n",
    "        \"improvement\": {\n",
    "            \"rouge1\": float(finetuned_rouge['rouge1'] - baseline_rouge['rouge1']),\n",
    "            \"rouge2\": float(finetuned_rouge['rouge2'] - baseline_rouge['rouge2']),\n",
    "            \"rougeL\": float(finetuned_rouge['rougeL'] - baseline_rouge['rougeL'])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(\"./qwen_lora_patent_real/metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(\"✓ Metadata saved\")\n",
    "\n",
    "# Save sample predictions for reference\n",
    "samples = []\n",
    "for i in range(min(5, len(test_refs))):\n",
    "    samples.append({\n",
    "        TITLE: dataset['test'][i][TITLE],\n",
    "        \"reference\": test_refs[i],\n",
    "        \"baseline\": baseline_preds[i],\n",
    "        \"finetuned\": finetuned_preds[i]\n",
    "    })\n",
    "\n",
    "with open(\"./qwen_lora_patent_real/sample_predictions.json\", \"w\") as f:\n",
    "    json.dump(samples, f, indent=2, ensure_ascii=False)\n",
    "print(\"✓ Sample predictions saved\")\n",
    "\n",
    "print(f\"\\n✓ Complete model package saved to: ./qwen_lora_patent_real/\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  - adapter_model.bin (~13MB)\")\n",
    "print(\"  - adapter_config.json\")\n",
    "print(\"  - tokenizer files\")\n",
    "print(\"  - metadata.json\")\n",
    "print(\"  - sample_predictions.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "FINAL RESULTS\n",
      "======================================================================\n",
      "Baseline ROUGE-L:   0.0362\n",
      "Fine-tuned ROUGE-L: 0.6993\n",
      "Change: +0.6631 (+66.31%)\n",
      "\n",
      "Detailed scores:\n",
      "  ROUGE-1: 0.0362 -> 0.7469\n",
      "  ROUGE-2: 0.0000 -> 0.6095\n",
      "  ROUGE-L: 0.0362 -> 0.6993\n",
      "\n",
      "=== Sample Comparisons ===\n",
      "\n",
      "--- Inspiration oxygen saver ---\n",
      "Reference:  A reliable, pulse-flow supplemental oxygen apparatus for alleviating respiratory ailments is provided which yields substantial savings in oxygen while giving the patient the physiological equivalent of a prescribed continuous stream of oxygen. The apparatus preferably includes a demand oxygen valve operated in a pulse mode by means of electronic control circuitry which, through an appropriate sensor, monitors the patient&#39;s breathing efforts and gives a variable &#34;custom tailored&#34; pulse volume of oxygen to the patient during the very initial stages of each inspiration.\n",
      "Baseline:   [Title of the Patent Document]  \n",
      "[Date of Publication]  \n",
      "[Patent Number]  \n",
      "\n",
      "[Abstract]  \n",
      "[Summary of the invention]  \n",
      "\n",
      "[Background]  \n",
      "[Context of the invention]  \n",
      "\n",
      "[Invention]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Advantages]  \n",
      "[Benefits of the invention]  \n",
      "\n",
      "[Patent Claims]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent Drawings]  \n",
      "[Detailed description of the invention]  \n",
      "\n",
      "[Patent\n",
      "Fine-tuned: A reliable, pulse-flow supplemental oxygen apparatus for alleviating respiratory ailments is provided which yields substantial savings in oxygen while giving the patient the physiological equivalent of a prescribed continuous stream of oxygen. The apparatus preferably includes a demand oxygen valve operated in a pulse mode by means of an electronic control circuitry which, through an appropriate sensor, monitors the patient's breathing efforts and gives a variable \"custom tailored\" pulse volume of oxygen to the patient during the very initial stages of each inspiration.\n",
      "\n",
      "--- Sawhorse ---\n",
      "Reference:  A kit for use with 2Ã—4 and 2Ã—6 wood pieces to make sawhorses. The legs are constructed for bent metal rods and are secured to brackets or support assemblies that are dimensioned to receive and support the wood piece. A minimal number of nails or screws for securing the wood to the brackets complete the manufacture. In addition, the legs are pivotally secured and may be folded against the wood pieces for storage or transport.\n",
      "Baseline:   [Title of the Patent Document]\n",
      "\n",
      "[Abstract of the Patent Document]\n",
      "\n",
      "[Background of the Patent Document]\n",
      "\n",
      "[Summary of the Patent Document]\n",
      "\n",
      "[Detailed Description of the Patent Document]\n",
      "\n",
      "[Claims of the Patent Document]\n",
      "\n",
      "[Drawings of the Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent Document]\n",
      "\n",
      "[Patent\n",
      "Fine-tuned: Claim: A kit for use with 2Ã—4 and 2Ã—6 wood pieces to make sawhorses, comprising a bracket and a leg secured to the bracket, wherein the leg is constructed for a bent metal rod and is secured to a mounting assembly with a plurality of pins and a cam mechanism causing the pin associated with each leg pivotally secure the two legs against the wood piece when bent.\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 9. Display Results\n",
    "# ================================================================\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Baseline ROUGE-L:   {baseline_rouge['rougeL']:.4f}\")\n",
    "print(f\"Fine-tuned ROUGE-L: {finetuned_rouge['rougeL']:.4f}\")\n",
    "delta = finetuned_rouge['rougeL'] - baseline_rouge['rougeL']\n",
    "print(f\"Change: {'+' if delta >= 0 else ''}{delta:.4f} ({delta*100:+.2f}%)\")\n",
    "\n",
    "print(f\"\\nDetailed scores:\")\n",
    "print(f\"  ROUGE-1: {baseline_rouge['rouge1']:.4f} -> {finetuned_rouge['rouge1']:.4f}\")\n",
    "print(f\"  ROUGE-2: {baseline_rouge['rouge2']:.4f} -> {finetuned_rouge['rouge2']:.4f}\")\n",
    "print(f\"  ROUGE-L: {baseline_rouge['rougeL']:.4f} -> {finetuned_rouge['rougeL']:.4f}\")\n",
    "\n",
    "print(\"\\n=== Sample Comparisons ===\")\n",
    "for i in range(min(3, len(test_refs))):\n",
    "    print(f\"\\n--- {dataset['test'][i][TITLE]} ---\")\n",
    "    print(f\"Reference:  {test_refs[i][:]}\")\n",
    "    print(f\"Baseline:   {baseline_preds[i][:]}\")\n",
    "    print(f\"Fine-tuned: {finetuned_preds[i][:]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING COMPLETE!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSC9010",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
