{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c4faef",
   "metadata": {},
   "source": [
    "# Required Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf49d13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "from peft import PeftModel\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from evaluate import load as load_metric\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class PrintStepCallback(TrainerCallback):\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        if state.global_step % 10 == 0:\n",
    "            loss = state.log_history[-1].get('loss', 'N/A') if state.log_history else 'N/A'\n",
    "            print(f\"Step {state.global_step}, Loss: {loss}\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ded3ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DEVICE: mps\n"
     ]
    }
   ],
   "source": [
    "target_device = ''\n",
    "if torch.backends.mps.is_available():\n",
    "    target_device = 'mps'\n",
    "elif torch .cuda.is_available():\n",
    "    target_device = 'cuda'\n",
    "else:\n",
    "    target_device = 'cpu'\n",
    "\n",
    "DEVICE = torch.device(target_device)\n",
    "print(\"Using DEVICE:\", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdcdfe9",
   "metadata": {},
   "source": [
    "# Constants/Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1cb386e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the incoming .json dataset\n",
    "TITLE = 'title'\n",
    "DESCRIPTION = 'description'\n",
    "SUMMARY = 'summary'\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B-Base\"\n",
    "\n",
    "DATA_PATH = \"./new-fifty-patents/patents_no_claims.json\"\n",
    "DATA_SLICE = None\n",
    "\n",
    "PROMPT_PATH = \"./prompt-construction/patent_prompts.json\"\n",
    "PROMPT_SLICE = 10 # Uses prompts[0:PROMPT_SLICE]. Set to None to use all prompts\n",
    "\n",
    "MODEL_PATH = \"./qwen_lora_patent_real\"\n",
    "\n",
    "# Model hyperparameters\n",
    "MAX_LENGTH = 512        # len\n",
    "MAX_TEXT_TOKENS = 350   # tt\n",
    "\n",
    "MAX_NEW_TOKENS = 1000   # nt\n",
    "EPOCHS = 30             # ep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b45eb8",
   "metadata": {},
   "source": [
    "# Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "230e36c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_row_text(s):\n",
    "    lst = ast.literal_eval(s)\n",
    "    json_str = json.dumps(lst)\n",
    "    obj = json.loads(json_str)\n",
    "    return obj[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eaa90ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, tokenizer, max_tokens):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a9d3fe35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch, tokenizer):\n",
    "    input_ids_list = []\n",
    "    attention_list = []\n",
    "    labels_list = []\n",
    "\n",
    "    for summary, description in zip(batch[SUMMARY], batch[DESCRIPTION]):\n",
    "        summary = truncate_text(summary, tokenizer, MAX_TEXT_TOKENS)\n",
    "        \n",
    "        # prompt = f\"Summarize this patent:\\n\\n{text}\\n\\nSummary: \"\n",
    "        prompt = f\"Generate a full detailed patent document based on this summary: \\n\\n{summary}\\n\\n Patent Document:\"\n",
    "        target = description + tokenizer.eos_token\n",
    "        full_text = prompt + target\n",
    "\n",
    "        target_ids = tokenizer.encode(target, add_special_tokens=False)\n",
    "        target_len = len(target_ids)\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=\"max_length\",\n",
    "            add_special_tokens=True,\n",
    "        )\n",
    "\n",
    "        input_ids = tokenized[\"input_ids\"]\n",
    "        attention_mask = tokenized[\"attention_mask\"]\n",
    "        seq_len = sum(attention_mask)\n",
    "\n",
    "        labels = [-100] * MAX_LENGTH\n",
    "        target_start = seq_len - target_len\n",
    "        \n",
    "        for i in range(target_len):\n",
    "            pos = target_start + i\n",
    "            if 0 <= pos < MAX_LENGTH:\n",
    "                labels[pos] = input_ids[pos]\n",
    "\n",
    "        input_ids_list.append(input_ids)\n",
    "        attention_list.append(attention_mask)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_list,\n",
    "        \"labels\": labels_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7370f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(mdl, text, tokenizer):\n",
    "    text = truncate_text(text, tokenizer, MAX_TEXT_TOKENS)\n",
    "    # prompt = f\"Summarize this patent:\\n\\n{text}\\n\\nSummary:\"\n",
    "    prompt = f\"Generate a full patent document based on this summary: \\n\\n{text}\\n\\n Patent Document:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")#.to(\"cuda\")\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}  # move to DEVICE (CPU or GPU)\n",
    "\n",
    "    mdl.eval()\n",
    "    with torch.no_grad():\n",
    "        output = mdl.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    full_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    if \"Patent Document:\" in full_output:\n",
    "        return full_output.split(\"Patent Document:\")[-1].strip()\n",
    "    return full_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "40449b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor([x[\"input_ids\"] for x in batch], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor([x[\"attention_mask\"] for x in batch], dtype=torch.long),\n",
    "        \"labels\": torch.tensor([x[\"labels\"] for x in batch], dtype=torch.long),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591c8e4d",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b2a02239",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_name = MODEL_NAME\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float32,  # or torch.bfloat16 if CUDA available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5c7e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen3ForCausalLM(\n",
       "      (model): Qwen3Model(\n",
       "        (embed_tokens): Embedding(151936, 1024)\n",
       "        (layers): ModuleList(\n",
       "          (0-27): 28 x Qwen3DecoderLayer(\n",
       "            (self_attn): Qwen3Attention(\n",
       "              (q_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=2048, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
       "            )\n",
       "            (mlp): Qwen3MLP(\n",
       "              (gate_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=3072, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n",
       "        (rotary_emb): Qwen3RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Load LoRA adapters on top of base model\n",
    "model = PeftModel.from_pretrained(model, MODEL_PATH)\n",
    "\n",
    "# 3. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "\n",
    "# 4. Move to device\n",
    "model.to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ac3199",
   "metadata": {},
   "source": [
    "# Load Data and Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "377e592c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 patents with real summaries\n"
     ]
    }
   ],
   "source": [
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "if DATA_SLICE:\n",
    "    data = data[:DATA_SLICE]\n",
    "\n",
    "print(f\"Loaded {len(data)} patents with real summaries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0fe6d8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 1: Motorized pet door apparatus\n",
      "  Summary: An electrically powered pet door assembly for use in a home or building. An apparatus as set forth in claim 6 wherein said trigger means includes a Sc...\n",
      "\n",
      "Example 2: Power line to drive a vehicle\n",
      "  Summary: A motor vehicle power line comprises a combustion engine (8), a gearbox (10) and an electronically controlled clutch (20) The engine throttle (84) is ...\n",
      "\n",
      "Example 3: Refrigerant reclaim method and apparatus\n",
      "  Summary: Refrigerant reclaim system includes a compressor, a heat exchanger, an oil separator, a condenser, a chill tank, a filter-dryer and a cooling coil in ...\n",
      "\n",
      "Total records: 50\n"
     ]
    }
   ],
   "source": [
    "records = []\n",
    "for i, item in enumerate(data):\n",
    "    if SUMMARY in item and DESCRIPTION in item:\n",
    "        records.append({\n",
    "            DESCRIPTION: item[DESCRIPTION],\n",
    "            TITLE: item[TITLE],\n",
    "            SUMMARY: item[SUMMARY]\n",
    "        })\n",
    "        if i < 3:\n",
    "            print(f\"\\nExample {i+1}: {item[TITLE]}\")\n",
    "            print(f\"  Summary: {item[SUMMARY][:150]}...\")\n",
    "\n",
    "print(f\"\\nTotal records: {len(records)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6375a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['description', 'title', 'summary'],\n",
      "    num_rows: 50\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 3. Create Dataset\n",
    "# ================================================================\n",
    "\n",
    "dataset = Dataset.from_list(records)\n",
    "# dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "# print(f\"\\nTrain: {len(dataset['train'])}, Test: {len(dataset['test'])}\")\n",
    "# print(dataset['train'])\n",
    "# print(dataset['test'][0])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b8e737b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 888.95 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid label tokens: 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# 4. Preprocessing\n",
    "# ================================================================\n",
    "tokenized_train = dataset.map(\n",
    "    lambda batch: preprocess(batch, tokenizer),\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names\n",
    ")\n",
    "\n",
    "example = tokenized_train[0]\n",
    "valid_count = sum(1 for l in example[\"labels\"] if l != -100)\n",
    "print(f\"Valid label tokens: {valid_count}\")\n",
    "\n",
    "summaries = [item[SUMMARY] for item in dataset]\n",
    "references = [item[DESCRIPTION] for item in dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa06676",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8bf02c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Load Evaluation Metric\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "# Test\n",
    "predictions = []\n",
    "comparisons = []\n",
    "for summary, ref in zip(summaries, references):\n",
    "    prompt = f\"Generate a full detailed patent document based on this summary: \\n\\n{summary}\\n\\n Patent Document:\"\n",
    "    pred = generate_summary(model, prompt, tokenizer)\n",
    "\n",
    "    finetuned_rouge = rouge.compute(predictions=[pred], references=[ref])\n",
    "    comparisons.append(finetuned_rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6d7d765f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'rouge1': np.float64(0.29464285714285715),\n",
       "  'rouge2': np.float64(0.07207207207207207),\n",
       "  'rougeL': np.float64(0.20535714285714285),\n",
       "  'rougeLsum': np.float64(0.23214285714285715)},\n",
       " {'rouge1': np.float64(0.5635359116022098),\n",
       "  'rouge2': np.float64(0.37222222222222223),\n",
       "  'rougeL': np.float64(0.3314917127071823),\n",
       "  'rougeLsum': np.float64(0.3314917127071823)},\n",
       " {'rouge1': np.float64(0.75625),\n",
       "  'rouge2': np.float64(0.5220125786163522),\n",
       "  'rougeL': np.float64(0.5125),\n",
       "  'rougeLsum': np.float64(0.5125)},\n",
       " {'rouge1': np.float64(0.477815699658703),\n",
       "  'rouge2': np.float64(0.3505154639175258),\n",
       "  'rougeL': np.float64(0.40955631399317405),\n",
       "  'rougeLsum': np.float64(0.40955631399317405)},\n",
       " {'rouge1': np.float64(0.351219512195122),\n",
       "  'rouge2': np.float64(0.2660098522167488),\n",
       "  'rougeL': np.float64(0.34146341463414637),\n",
       "  'rougeLsum': np.float64(0.34146341463414637)},\n",
       " {'rouge1': np.float64(0.24242424242424246),\n",
       "  'rouge2': np.float64(0.03125),\n",
       "  'rougeL': np.float64(0.12121212121212123),\n",
       "  'rougeLsum': np.float64(0.12121212121212123)},\n",
       " {'rouge1': np.float64(0.41379310344827586),\n",
       "  'rouge2': np.float64(0.11940298507462688),\n",
       "  'rougeL': np.float64(0.2266009852216749),\n",
       "  'rougeLsum': np.float64(0.2266009852216749)},\n",
       " {'rouge1': np.float64(0.5826771653543308),\n",
       "  'rouge2': np.float64(0.42063492063492064),\n",
       "  'rougeL': np.float64(0.4724409448818898),\n",
       "  'rougeLsum': np.float64(0.4724409448818898)},\n",
       " {'rouge1': np.float64(0.32876712328767127),\n",
       "  'rouge2': np.float64(0.21379310344827585),\n",
       "  'rougeL': np.float64(0.23287671232876714),\n",
       "  'rougeLsum': np.float64(0.23287671232876714)},\n",
       " {'rouge1': np.float64(0.5258964143426295),\n",
       "  'rouge2': np.float64(0.36144578313253006),\n",
       "  'rougeL': np.float64(0.4302788844621514),\n",
       "  'rougeLsum': np.float64(0.4302788844621514)},\n",
       " {'rouge1': np.float64(0.6578947368421053),\n",
       "  'rouge2': np.float64(0.5945945945945945),\n",
       "  'rougeL': np.float64(0.6578947368421053),\n",
       "  'rougeLsum': np.float64(0.6578947368421053)},\n",
       " {'rouge1': np.float64(0.38626609442060084),\n",
       "  'rouge2': np.float64(0.23376623376623376),\n",
       "  'rougeL': np.float64(0.32618025751072965),\n",
       "  'rougeLsum': np.float64(0.32618025751072965)},\n",
       " {'rouge1': np.float64(0.6692913385826771),\n",
       "  'rouge2': np.float64(0.5158730158730159),\n",
       "  'rougeL': np.float64(0.5748031496062993),\n",
       "  'rougeLsum': np.float64(0.5748031496062993)},\n",
       " {'rouge1': np.float64(0.7200000000000001),\n",
       "  'rouge2': np.float64(0.6849315068493151),\n",
       "  'rougeL': np.float64(0.7200000000000001),\n",
       "  'rougeLsum': np.float64(0.7200000000000001)},\n",
       " {'rouge1': np.float64(0.42580645161290326),\n",
       "  'rouge2': np.float64(0.2142857142857143),\n",
       "  'rougeL': np.float64(0.2838709677419355),\n",
       "  'rougeLsum': np.float64(0.2838709677419355)},\n",
       " {'rouge1': np.float64(0.3983050847457627),\n",
       "  'rouge2': np.float64(0.09401709401709402),\n",
       "  'rougeL': np.float64(0.23728813559322032),\n",
       "  'rougeLsum': np.float64(0.23728813559322032)},\n",
       " {'rouge1': np.float64(0.6388888888888888),\n",
       "  'rouge2': np.float64(0.5774647887323944),\n",
       "  'rougeL': np.float64(0.5972222222222222),\n",
       "  'rougeLsum': np.float64(0.5972222222222222)},\n",
       " {'rouge1': np.float64(0.12903225806451613),\n",
       "  'rouge2': np.float64(0.12093023255813953),\n",
       "  'rougeL': np.float64(0.12903225806451613),\n",
       "  'rougeLsum': np.float64(0.12903225806451613)},\n",
       " {'rouge1': np.float64(0.35761589403973515),\n",
       "  'rouge2': np.float64(0.21476510067114096),\n",
       "  'rougeL': np.float64(0.2119205298013245),\n",
       "  'rougeLsum': np.float64(0.2119205298013245)},\n",
       " {'rouge1': np.float64(0.4719101123595505),\n",
       "  'rouge2': np.float64(0.3924528301886792),\n",
       "  'rougeL': np.float64(0.4119850187265918),\n",
       "  'rougeLsum': np.float64(0.4119850187265918)},\n",
       " {'rouge1': np.float64(0.6625386996904025),\n",
       "  'rouge2': np.float64(0.4610591900311527),\n",
       "  'rougeL': np.float64(0.5263157894736842),\n",
       "  'rougeLsum': np.float64(0.5263157894736842)},\n",
       " {'rouge1': np.float64(0.2037735849056604),\n",
       "  'rouge2': np.float64(0.12927756653992395),\n",
       "  'rougeL': np.float64(0.14339622641509434),\n",
       "  'rougeLsum': np.float64(0.17358490566037735)},\n",
       " {'rouge1': np.float64(0.5123456790123457),\n",
       "  'rouge2': np.float64(0.3726708074534162),\n",
       "  'rougeL': np.float64(0.42592592592592593),\n",
       "  'rougeLsum': np.float64(0.42592592592592593)},\n",
       " {'rouge1': np.float64(0.28187919463087246),\n",
       "  'rouge2': np.float64(0.10884353741496598),\n",
       "  'rougeL': np.float64(0.20134228187919467),\n",
       "  'rougeLsum': np.float64(0.20134228187919467)},\n",
       " {'rouge1': np.float64(0.7674418604651162),\n",
       "  'rouge2': np.float64(0.761904761904762),\n",
       "  'rougeL': np.float64(0.7674418604651162),\n",
       "  'rougeLsum': np.float64(0.7674418604651162)},\n",
       " {'rouge1': np.float64(0.6233766233766234),\n",
       "  'rouge2': np.float64(0.3790849673202614),\n",
       "  'rougeL': np.float64(0.38311688311688313),\n",
       "  'rougeLsum': np.float64(0.38311688311688313)},\n",
       " {'rouge1': np.float64(0.43548387096774194),\n",
       "  'rouge2': np.float64(0.30894308943089427),\n",
       "  'rougeL': np.float64(0.3870967741935483),\n",
       "  'rougeLsum': np.float64(0.3870967741935483)},\n",
       " {'rouge1': np.float64(0.5290322580645161),\n",
       "  'rouge2': np.float64(0.3660130718954248),\n",
       "  'rougeL': np.float64(0.4),\n",
       "  'rougeLsum': np.float64(0.4)},\n",
       " {'rouge1': np.float64(0.33587786259541985),\n",
       "  'rouge2': np.float64(0.031007751937984496),\n",
       "  'rougeL': np.float64(0.19847328244274812),\n",
       "  'rougeLsum': np.float64(0.29007633587786263)},\n",
       " {'rouge1': np.float64(0.9913043478260869),\n",
       "  'rouge2': np.float64(0.9911504424778761),\n",
       "  'rougeL': np.float64(0.9913043478260869),\n",
       "  'rougeLsum': np.float64(0.9913043478260869)},\n",
       " {'rouge1': np.float64(0.4576271186440678),\n",
       "  'rouge2': np.float64(0.40170940170940167),\n",
       "  'rougeL': np.float64(0.4152542372881356),\n",
       "  'rougeLsum': np.float64(0.4152542372881356)},\n",
       " {'rouge1': np.float64(0.61),\n",
       "  'rouge2': np.float64(0.46464646464646464),\n",
       "  'rougeL': np.float64(0.49000000000000005),\n",
       "  'rougeLsum': np.float64(0.49000000000000005)},\n",
       " {'rouge1': np.float64(0.7311827956989247),\n",
       "  'rouge2': np.float64(0.6153846153846153),\n",
       "  'rougeL': np.float64(0.5806451612903226),\n",
       "  'rougeLsum': np.float64(0.5806451612903226)},\n",
       " {'rouge1': np.float64(0.5060240963855421),\n",
       "  'rouge2': np.float64(0.3658536585365854),\n",
       "  'rougeL': np.float64(0.42168674698795183),\n",
       "  'rougeLsum': np.float64(0.42168674698795183)},\n",
       " {'rouge1': np.float64(0.6183574879227053),\n",
       "  'rouge2': np.float64(0.41951219512195115),\n",
       "  'rougeL': np.float64(0.5217391304347826),\n",
       "  'rougeLsum': np.float64(0.5217391304347826)},\n",
       " {'rouge1': np.float64(0.9113924050632911),\n",
       "  'rouge2': np.float64(0.8311688311688312),\n",
       "  'rougeL': np.float64(0.8607594936708861),\n",
       "  'rougeLsum': np.float64(0.8607594936708861)},\n",
       " {'rouge1': np.float64(0.5991189427312776),\n",
       "  'rouge2': np.float64(0.4177777777777778),\n",
       "  'rougeL': np.float64(0.4405286343612335),\n",
       "  'rougeLsum': np.float64(0.4405286343612335)},\n",
       " {'rouge1': np.float64(0.254416961130742),\n",
       "  'rouge2': np.float64(0.04270462633451957),\n",
       "  'rougeL': np.float64(0.15547703180212014),\n",
       "  'rougeLsum': np.float64(0.15547703180212014)},\n",
       " {'rouge1': np.float64(0.180327868852459),\n",
       "  'rouge2': np.float64(0.016483516483516484),\n",
       "  'rougeL': np.float64(0.11475409836065575),\n",
       "  'rougeLsum': np.float64(0.13114754098360656)},\n",
       " {'rouge1': np.float64(0.5067567567567568),\n",
       "  'rouge2': np.float64(0.40136054421768713),\n",
       "  'rougeL': np.float64(0.4189189189189189),\n",
       "  'rougeLsum': np.float64(0.4527027027027027)},\n",
       " {'rouge1': np.float64(0.5673758865248227),\n",
       "  'rouge2': np.float64(0.5035971223021584),\n",
       "  'rougeL': np.float64(0.5531914893617021),\n",
       "  'rougeLsum': np.float64(0.5531914893617021)},\n",
       " {'rouge1': np.float64(0.4558404558404558),\n",
       "  'rouge2': np.float64(0.2292263610315186),\n",
       "  'rougeL': np.float64(0.3475783475783476),\n",
       "  'rougeLsum': np.float64(0.3475783475783476)},\n",
       " {'rouge1': np.float64(0.4907975460122698),\n",
       "  'rouge2': np.float64(0.27104722792607805),\n",
       "  'rougeL': np.float64(0.3394683026584867),\n",
       "  'rougeLsum': np.float64(0.3394683026584867)},\n",
       " {'rouge1': np.float64(0.5952380952380952),\n",
       "  'rouge2': np.float64(0.376),\n",
       "  'rougeL': np.float64(0.42063492063492064),\n",
       "  'rougeLsum': np.float64(0.42063492063492064)},\n",
       " {'rouge1': np.float64(0.5441176470588236),\n",
       "  'rouge2': np.float64(0.49253731343283585),\n",
       "  'rougeL': np.float64(0.5294117647058825),\n",
       "  'rougeLsum': np.float64(0.5294117647058825)},\n",
       " {'rouge1': np.float64(0.4697508896797153),\n",
       "  'rouge2': np.float64(0.3225806451612903),\n",
       "  'rougeL': np.float64(0.35587188612099646),\n",
       "  'rougeLsum': np.float64(0.35587188612099646)},\n",
       " {'rouge1': np.float64(0.5954198473282443),\n",
       "  'rouge2': np.float64(0.5116279069767442),\n",
       "  'rougeL': np.float64(0.534351145038168),\n",
       "  'rougeLsum': np.float64(0.534351145038168)},\n",
       " {'rouge1': np.float64(0.3269230769230769),\n",
       "  'rouge2': np.float64(0.11764705882352941),\n",
       "  'rougeL': np.float64(0.23076923076923073),\n",
       "  'rougeLsum': np.float64(0.23076923076923073)},\n",
       " {'rouge1': np.float64(0.393939393939394),\n",
       "  'rouge2': np.float64(0.34375),\n",
       "  'rougeL': np.float64(0.393939393939394),\n",
       "  'rougeLsum': np.float64(0.393939393939394)},\n",
       " {'rouge1': np.float64(0.9206349206349206),\n",
       "  'rouge2': np.float64(0.8852459016393442),\n",
       "  'rougeL': np.float64(0.9206349206349206),\n",
       "  'rougeLsum': np.float64(0.9206349206349206)}]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparisons"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CSC9010",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
